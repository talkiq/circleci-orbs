version: 2.1
description: "Tools for running documentation commands"

jobs:
  compile-and-upload:
    description: >
      Compiles Sphinxdocs for a repository, then deploy it to Google Storage
      (ie. for internal use only). Optional hooks are provided for building
      APIdocs, etc.
    docker:
      - image: <<parameters.executor>>
    resource_class: <<parameters.resource_class>>
    parameters:
      creds:
        default: GCLOUD_SERVICE_KEY
        description: >
          Name of environment variable storing the base64-encoded service key
          for the GCP project.
        type: env_var_name
      executor:
        default: python:latest
        description: >
          Name of the docker image to use to execute the job. Must have python3
          installed.
        type: string
      folder:
        default: '.'
        description: >
          Path to the folder containing the Sphinx entrypoint (eg. the
          `conf.py` and `index.rst`).
        type: string
      gcp_project:
        description: The Google project ID to connect with via the gcloud CLI.
        type: string
      install:
        description: >
          If there are any packages which must be installed before running
          `sphinx-build`, add them here. It's like the case that this should
          match the `preprocess` step; eg. if you are generating APIdocs for
          source folders, you will need to have those packages (and their
          dependencies!) be importable by sphinx.

          In other words, you may want to set this value to something like:
          ```
          - run: pip install -e ./module1/src/
          - run: pip install -e ./module2/src/
          - run: pip install -r ./application1/src/requirements.txt
          ```
        type: steps
        default: []
      preprocess:
        description: >
          Any number of (optional) pre-processing steps for generating Sphinx
          documentation. A common use-case here is to pass in `sphinx-apidoc`
          commands. For a simple project with a single source directory, this
          might look something like:
          ```
          - run: sphinx-apidoc -M -e -f -o ./docs/code/ ./src/
          ```

          For a mono-repo, you may provide multiple steps, for example:
          ```
          - run: sphinx-apidoc -M -e -f -o ./docs/code/ ./module1/src/
          - run: sphinx-apidoc -M -e -f -o ./docs/code/ ./module2/src/
          - run: sphinx-apidoc -M -e -f -o ./docs/code/ ./module3/src/
          ```

          Note that you must do any cleanup you think is necessary: for
          example, in a monorepo you will likely want to remove the
          `modules.rst` file, which is overriden on each run:
          ```
          - run: rm ./docs/code/modules.rst
          ```
        type: steps
        default: []
      project:
        default: ${CIRCLE_PROJECT_REPONAME}
        description: |
          The name of the project folder in GCS.
        type: string
      resource_class:
        default: small
        type: string
    steps:
      - run: python3 -m pip install sphinx
      - checkout
      - steps: <<parameters.preprocess>>
      - steps: <<parameters.install>>
      - run: sphinx-build -b html -W --keep-going <<parameters.folder>>/ build/
      # TODO: this should probably get moved to eg. `gcloud/deploy-to-gcs`
      - gcloud/install
      - gcloud/auth:
          creds: <<parameters.creds>>
          project: <<parameters.gcp_project>>
      - run:
          name: nuke pre-existing docs
          command: |
            gsutil -m rm -r gs://<<parameters.gcp_project>>-docs/<<parameters.project>>/ ||:
      - run: gsutil -m cp -r build/* gs://<<parameters.gcp_project>>-docs/<<parameters.project>>/
      # END TODO

orbs:
  gcloud: talkiq/gcloud@7
